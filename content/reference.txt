[1] XAI 課本 Explainable AI: Interpreting, Explaining and Visualizing Deep Learning  https://link.springer.com/book/10.1007/978-3-030-28954-6
[3] 隱藏性骨折 Development and Validation of a Deep Learning Model Using Convolutional Neural Networks to Identify Scaphoid Fractures in Radiographs https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2779578
[4] 自然語言處理對抗攻擊 TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP https://arxiv.org/abs/2005.05909
[5] 語音識別對抗攻擊 Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems https://arxiv.org/abs/1904.05734
[6] 歐盟的可信賴性人工智慧倫理準則 
Ethics Guidelines for Trustworthy AI https://ec.europa.eu/futurium/en/ai-alliance-consultation.1.html
[7] 美國的演算法課責法H.R.2231 - Algorithmic Accountability Act of 2019 https://www.congress.gov/bill/116th-congress/house-bill/2231
[8]  Gradient-Based Learning Applied to Document Recognition https://ieeexplore.ieee.org/document/726791
[9] CNN 範例圖 https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
[10] Very Deep Convolutional Networks for Large-Scale Image Recognition : https://arxiv.org/abs/1409.1556
[11] ImageNet Classification with Deep Convolutional Neural Networks : https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
//[12] AlexNet 架構圖 http://d2l.ai/chapter_convolutional-modern/alexnet.html
[13] “Why Should I Trust You?” Explaining the Predictions of Any Classifier — Ribeiro, Singh, Guestrin (2016) https://arxiv.org/abs/1602.04938
[14] The Human Rights, Big Data and Technology Project - written evidence (https://committees.parliament.uk/writtenevidence/445/pdf/)
[15] Visualizing and Understanding Convolutional Networks https://arxiv.org/abs/1311.2901
[16] Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps https://arxiv.org/abs/1312.6034
[17] How to Explain Individual Classification Decisions https://arxiv.org/abs/0912.1128
[18] Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps https://arxiv.org/abs/1312.6034
[19] Not Just a Black Box: Learning Important Features Through Propagating Activation Differences https://arxiv.org/abs/1605.01713
[20] Learning Important Features Through Propagating Activation Differences https://arxiv.org/abs/1704.02685
[21] Axiomatic Attribution for Deep Networks https://arxiv.org/abs/1703.01365
[22] On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140
[23] Explaining NonLinear Classification Decisions with Deep Taylor Decomposition https://arxiv.org/abs/1512.02479
[24] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving for simplicity: The all convolutional net. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings, 2015. https://arxiv.org/abs/1412.6806
[25] Intriguing properties of neural networks ( https://arxiv.org/abs/1312.6199)
[26] J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” arXiv.org, 20-Mar-2015. [Online]. Available: https://arxiv.org/abs/1412.6572. 
[27] Interpretation of Neural Networks is Fragile https://arxiv.org/abs/1710.10547
[28] THE (UN)RELIABILITY OF SALIENCY METHODS https://arxiv.org/abs/1711.00867
[29] Sanity Checks for Saliency Maps https://arxiv.org/abs/1810.03292
[30] Fooling Neural Network Interpretations via Adversarial Model Manipulation https://arxiv.org/abs/1902.02041
[31] Explanations can be manipulated and geometry is to blame https://arxiv.org/abs/1906.07983
[32] Comparing Measures of Sparsity https://arxiv.org/abs/0811.4706
[33] What the success of brain imaging implies about the neural code https://elifesciences.org/articles/21397
[2] Cifar10 https://www.cs.toronto.edu/~kriz/cifar.html